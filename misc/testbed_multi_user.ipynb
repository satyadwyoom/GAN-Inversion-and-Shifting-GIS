{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import random\n",
    "from tqdm import *\n",
    "from PIL import Image\n",
    "from io import StringIO, BytesIO\n",
    "import lpips\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from pro_gan_pytorch.networks import create_generator_from_saved_model, create_generator_from_saved_model_opt\n",
    "from pro_gan_pytorch.utils import adjust_dynamic_range\n",
    "from torch.nn.functional import interpolate\n",
    "import torchvision.transforms.functional as fn\n",
    "from train_log import MeanTracker\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import model_rs as recsys_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is Available:  True\n",
      "Total number of Train Samples:  166270\n"
     ]
    }
   ],
   "source": [
    "device = 1\n",
    "seed = 0\n",
    "\n",
    "data_train = 'amazon'\n",
    "out_dir = 'dummy_rec/'\n",
    "latent_dim = 512\n",
    "learning_rate = 0.00002\n",
    "training_epoch = 1000\n",
    "batch_size = 1\n",
    "numofworkers= 4\n",
    "gan_weights= '../PROGAN_AM_Fashion/Model_log_base/models/depth_7_epoch_50.bin' \n",
    "\n",
    "#torch.cuda.set_device(device)\n",
    "print('Cuda is Available: ', torch.cuda.is_available())\n",
    "random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "\n",
    "# print('Seed val fixed to: ', seed)\n",
    "\n",
    "\n",
    "if data_train == 'amazon':\n",
    "\n",
    "    dataset_name = 'AmazonFashion6ImgPartitioned.npy'\n",
    "    dataset = np.load('../DVBPR/dataset/'+ dataset_name, encoding='bytes', allow_pickle=True)\n",
    "    [user_train, _, _, Item, usernum, itemnum] = dataset\n",
    "\n",
    "elif data_train == 'tradesy':\n",
    "\n",
    "    dataset_name = 'TradesyImgPartitioned.npy'\n",
    "    dataset = np.load('../DVBPR/data/' + dataset_name, encoding='bytes')\n",
    "    [user_train, user_validation, user_test, Item, usernum, itemnum] = dataset\n",
    "    cold_list = np.load('../data/tradesy_one_k_cold.npy')\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    img_pil =  Image.open(BytesIO(path)).convert('RGB')\n",
    "    img_tensor = input_transform(img_pil)\n",
    "    return img_tensor\n",
    "\n",
    "# input_transform = transforms.Compose([\n",
    "#     transforms.Resize((128, 128)),\n",
    "#     transforms.ToTensor(),\n",
    "#     # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#     #                     std=[0.229, 0.224, 0.225])\n",
    "#     # transforms.Normalize((0.6949, 0.6748, 0.6676), (0.3102, 0.3220, 0.3252))])\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
    "#     ])\n",
    "\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "\n",
    "class trainset(Dataset):\n",
    "    def __init__(self, loader=default_loader):\n",
    "        self.images_i = file_train_i\n",
    "        self.images_j = file_train_j\n",
    "        self.target = train_ls\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn_i = self.images_i[index]\n",
    "        img_i = self.loader(fn_i)\n",
    "        fn_j = self.images_j[index]\n",
    "        img_j = self.loader(fn_j)\n",
    "        target = self.target[index]\n",
    "        return img_i, img_j, target[0], target[1], target[2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_i)\n",
    "\n",
    "\n",
    "\n",
    "# helper scale function\n",
    "def scale(x):\n",
    "    # assume x is scaled to (0, 1)\n",
    "    # scale to feature_range and return scaled x\n",
    "    x = (x - x.min()) / (x.max() - x.min())\n",
    "    return x\n",
    "\n",
    "def scale_synth(x):\n",
    "    # scale to feature_range and return scaled x\n",
    "    x = adjust_dynamic_range(x,drange_in=(-1.0, 1.0), drange_out=(0.0, 1.0))\n",
    "    return x\n",
    "\n",
    "def scale_percept(x):\n",
    "    # scale to feature_range and return scaled x\n",
    "    x = adjust_dynamic_range(x,drange_in=(-1.0, 1.0), drange_out=(-1.0, 1.0))\n",
    "\n",
    "    return x\n",
    "\n",
    "def manual_normalize(x, mean, std):\n",
    "    mean_ten = torch.Tensor(mean).unsqueeze(dim=0).unsqueeze(dim=2).unsqueeze(dim=3).cuda()\n",
    "    std_ten = torch.Tensor(std).unsqueeze(dim=0).unsqueeze(dim=2).unsqueeze(dim=3).cuda()\n",
    "    x = (x-mean_ten)/std_ten\n",
    "    return x\n",
    "\n",
    "def scale_rs(x):\n",
    "    x = adjust_dynamic_range(x,drange_in=(-1.0, 1.0), drange_out=(0.0, 1.0))\n",
    "    x = interpolate(x, size=(224, 224), mode='bilinear')\n",
    "    x = manual_normalize(x, mean=[0.6949, 0.6748, 0.6676], std=[0.3102, 0.3220, 0.3252])\n",
    "    return x\n",
    "\n",
    "\n",
    "##### Initialize Dataset Object ######\n",
    "def sample(user):\n",
    "    u = random.randrange(usernum)\n",
    "    numu = len(user[u])\n",
    "    i = user[u][random.randrange(numu)][b'productid']\n",
    "    M=set()\n",
    "    for item in user[u]:\n",
    "        M.add(item[b'productid'])\n",
    "    while True:\n",
    "        j=random.randrange(itemnum)\n",
    "        if (not j in M): break\n",
    "    return (u,i,j)\n",
    "\n",
    "oneiteration = 0\n",
    "for item in user_train: oneiteration+=len(user_train[item])\n",
    "\n",
    "train_ls = [list(sample(user_train)) for _ in range(oneiteration)]\n",
    "\n",
    "\n",
    "file_train_i = [Item[i][b'imgs'] for i in range(itemnum)]\n",
    "file_train_j = [Item[i][b'imgs'] for i in range(itemnum)]\n",
    "\n",
    "\n",
    "train_dataset  = trainset()\n",
    "\n",
    "data_loader_train = DataLoader(train_dataset, batch_size = 10, shuffle=True, num_workers = numofworkers, drop_last=True)\n",
    "print(\"Total number of Train Samples: \",len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loaded!!\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skumar40/anaconda3/envs/Rs_attack/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/skumar40/anaconda3/envs/Rs_attack/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/skumar40/anaconda3/envs/Rs_attack/lib/python3.8/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "class Progan_gen(nn.Module):\n",
    "    def __init__(self, weights_root):\n",
    "        super(Progan_gen, self).__init__()\n",
    "        # self.G = create_generator_from_saved_model(weights_root)\n",
    "        self.G = create_generator_from_saved_model_opt(weights_root)\n",
    "\n",
    "        self.G_curr_depth = 7\n",
    "        self.Actual_G_depth = 8\n",
    "\n",
    "    def forward(self, z, delta):\n",
    "        out = self.G(z, delta, self.G_curr_depth)\n",
    "        return out\n",
    "\n",
    "    # def gen_shifted(self, z, shift, delta):\n",
    "    #     return self.forward(z + shift, [i+shift.unsqueeze(dim=1).unsqueeze(dim=2) for i in delta])\n",
    "\n",
    "    def gen_shifted(self, z, shift, delta):\n",
    "        return self.forward(z + shift, delta)\n",
    "\n",
    "\n",
    "G = Progan_gen(gan_weights).cuda()\n",
    "G = G.eval()\n",
    "\n",
    "\n",
    "# class Progan_gen_1(nn.Module):\n",
    "#     def __init__(self, weights_root):\n",
    "#         super(Progan_gen_1, self).__init__()\n",
    "#         # self.G = create_generator_from_saved_model(weights_root)\n",
    "#         self.G = create_generator_from_saved_model(weights_root)\n",
    "\n",
    "#         self.G_curr_depth = 7\n",
    "#         self.Actual_G_depth = 8\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         out = self.G(z, self.G_curr_depth)\n",
    "#         return out\n",
    "\n",
    "#     # def gen_shifted(self, z, shift, delta):\n",
    "#     #     return self.forward(z + shift, [i+shift.unsqueeze(dim=1).unsqueeze(dim=2) for i in delta])\n",
    "\n",
    "#     def gen_shifted(self, z, shift):\n",
    "#         return self.forward(z + shift)\n",
    "\n",
    "\n",
    "# G = Progan_gen_1(gan_weights).cuda()\n",
    "# G = G.eval()\n",
    "\n",
    "print('Generator Loaded!!')\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "percept = lpips.LPIPS(net='vgg').cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shift-predictor Loaded!\n",
      "Working on Recommender Models!!\n",
      "Loading Rs Model Weights!!!!\n",
      "#user: 5 iter0: loss: 2.00| logit_loss: 1.91| shift_loss: 0.49| Preference_score: -1.32\n",
      "#user: 5 iter100: loss: -1.09| logit_loss: 0.03| shift_loss: 0.02| Preference_score: 1.12\n",
      "#user: 5 iter200: loss: -1.32| logit_loss: 0.02| shift_loss: 0.02| Preference_score: 1.33\n",
      "#user: 5 iter300: loss: -1.63| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 1.65\n",
      "#user: 5 iter400: loss: -1.83| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 1.84\n",
      "#user: 5 iter500: loss: -1.99| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 2.00\n",
      "#user: 5 iter600: loss: -2.11| logit_loss: 0.01| shift_loss: 0.02| Preference_score: 2.13\n",
      "#user: 5 iter700: loss: -2.05| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 2.06\n",
      "#user: 5 iter800: loss: -2.33| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 2.34\n",
      "#user: 5 iter900: loss: -2.28| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 2.28\n",
      "Shift-predictor Loaded!\n",
      "Working on Recommender Models!!\n",
      "Loading Rs Model Weights!!!!\n",
      "#user: 5 iter0: loss: 3.36| logit_loss: 1.85| shift_loss: 0.48| Preference_score: -2.70\n",
      "#user: 5 iter100: loss: 0.44| logit_loss: 0.02| shift_loss: 0.02| Preference_score: -0.42\n",
      "#user: 5 iter200: loss: -0.13| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.15\n",
      "#user: 5 iter300: loss: -0.17| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.19\n",
      "#user: 5 iter400: loss: -0.58| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.59\n",
      "#user: 5 iter500: loss: -0.83| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.84\n",
      "#user: 5 iter600: loss: -1.08| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 1.09\n",
      "#user: 5 iter700: loss: -0.96| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.97\n",
      "#user: 5 iter800: loss: -0.99| logit_loss: 0.00| shift_loss: 0.01| Preference_score: 1.00\n",
      "#user: 5 iter900: loss: -1.29| logit_loss: 0.00| shift_loss: 0.01| Preference_score: 1.30\n",
      "Shift-predictor Loaded!\n",
      "Working on Recommender Models!!\n",
      "Loading Rs Model Weights!!!!\n",
      "#user: 5 iter0: loss: 2.20| logit_loss: 1.83| shift_loss: 0.46| Preference_score: -1.55\n",
      "#user: 5 iter100: loss: -0.25| logit_loss: 0.01| shift_loss: 0.02| Preference_score: 0.27\n",
      "#user: 5 iter200: loss: -0.45| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.47\n",
      "#user: 5 iter300: loss: -0.56| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.57\n",
      "#user: 5 iter400: loss: -0.56| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.57\n",
      "#user: 5 iter500: loss: -0.62| logit_loss: 0.01| shift_loss: 0.02| Preference_score: 0.64\n",
      "#user: 5 iter600: loss: -0.69| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.70\n",
      "#user: 5 iter700: loss: -0.87| logit_loss: 0.00| shift_loss: 0.01| Preference_score: 0.88\n",
      "#user: 5 iter800: loss: -0.86| logit_loss: 0.00| shift_loss: 0.01| Preference_score: 0.87\n",
      "#user: 5 iter900: loss: -0.94| logit_loss: 0.00| shift_loss: 0.01| Preference_score: 0.95\n",
      "Shift-predictor Loaded!\n",
      "Working on Recommender Models!!\n",
      "Loading Rs Model Weights!!!!\n",
      "#user: 5 iter0: loss: 3.35| logit_loss: 2.04| shift_loss: 0.47| Preference_score: -2.68\n",
      "#user: 5 iter100: loss: 0.69| logit_loss: 0.02| shift_loss: 0.02| Preference_score: -0.67\n",
      "#user: 5 iter200: loss: 0.23| logit_loss: 0.02| shift_loss: 0.02| Preference_score: -0.20\n",
      "#user: 5 iter300: loss: -0.08| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.09\n",
      "#user: 5 iter400: loss: -0.30| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.32\n",
      "#user: 5 iter500: loss: -0.48| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.50\n",
      "#user: 5 iter600: loss: -0.57| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.58\n",
      "#user: 5 iter700: loss: -0.66| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.68\n",
      "#user: 5 iter800: loss: -0.96| logit_loss: 0.00| shift_loss: 0.01| Preference_score: 0.96\n",
      "#user: 5 iter900: loss: -1.03| logit_loss: 0.00| shift_loss: 0.01| Preference_score: 1.04\n",
      "Shift-predictor Loaded!\n",
      "Working on Recommender Models!!\n",
      "Loading Rs Model Weights!!!!\n",
      "#user: 5 iter0: loss: 2.85| logit_loss: 1.91| shift_loss: 0.50| Preference_score: -2.15\n",
      "#user: 5 iter100: loss: -0.17| logit_loss: 0.01| shift_loss: 0.04| Preference_score: 0.21\n",
      "#user: 5 iter200: loss: -0.56| logit_loss: 0.01| shift_loss: 0.02| Preference_score: 0.58\n",
      "#user: 5 iter300: loss: -0.76| logit_loss: 0.01| shift_loss: 0.02| Preference_score: 0.78\n",
      "#user: 5 iter400: loss: -1.04| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 1.05\n",
      "#user: 5 iter500: loss: -1.27| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 1.29\n",
      "#user: 5 iter600: loss: -1.21| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 1.22\n",
      "#user: 5 iter700: loss: -1.25| logit_loss: 0.00| shift_loss: 0.01| Preference_score: 1.26\n",
      "#user: 5 iter800: loss: -1.34| logit_loss: 0.00| shift_loss: 0.01| Preference_score: 1.35\n",
      "#user: 5 iter900: loss: -1.46| logit_loss: 0.00| shift_loss: 0.01| Preference_score: 1.47\n",
      "Shift-predictor Loaded!\n",
      "Working on Recommender Models!!\n",
      "Loading Rs Model Weights!!!!\n",
      "#user: 5 iter0: loss: 4.45| logit_loss: 1.94| shift_loss: 0.50| Preference_score: -3.76\n",
      "#user: 5 iter100: loss: 2.36| logit_loss: 0.04| shift_loss: 0.06| Preference_score: -2.30\n",
      "#user: 5 iter200: loss: 1.94| logit_loss: 0.01| shift_loss: 0.01| Preference_score: -1.93\n",
      "#user: 5 iter300: loss: 1.86| logit_loss: 0.01| shift_loss: 0.01| Preference_score: -1.85\n",
      "#user: 5 iter400: loss: 1.78| logit_loss: 0.01| shift_loss: 0.01| Preference_score: -1.77\n",
      "#user: 5 iter500: loss: 1.75| logit_loss: 0.01| shift_loss: 0.01| Preference_score: -1.74\n",
      "#user: 5 iter600: loss: 1.61| logit_loss: 0.01| shift_loss: 0.01| Preference_score: -1.60\n",
      "#user: 5 iter700: loss: 1.65| logit_loss: 0.01| shift_loss: 0.01| Preference_score: -1.65\n",
      "#user: 5 iter800: loss: 1.56| logit_loss: 0.01| shift_loss: 0.01| Preference_score: -1.55\n",
      "#user: 5 iter900: loss: 1.63| logit_loss: 0.00| shift_loss: 0.01| Preference_score: -1.62\n",
      "Shift-predictor Loaded!\n",
      "Working on Recommender Models!!\n",
      "Loading Rs Model Weights!!!!\n",
      "#user: 5 iter0: loss: 3.18| logit_loss: 1.87| shift_loss: 0.48| Preference_score: -2.51\n",
      "#user: 5 iter100: loss: 0.64| logit_loss: 0.02| shift_loss: 0.02| Preference_score: -0.62\n",
      "#user: 5 iter200: loss: 0.38| logit_loss: 0.01| shift_loss: 0.02| Preference_score: -0.37\n",
      "#user: 5 iter300: loss: 0.12| logit_loss: 0.01| shift_loss: 0.01| Preference_score: -0.10\n",
      "#user: 5 iter400: loss: -0.09| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.11\n",
      "#user: 5 iter500: loss: -0.17| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.18\n",
      "#user: 5 iter600: loss: -0.13| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 0.14\n",
      "#user: 5 iter700: loss: -0.26| logit_loss: 0.00| shift_loss: 0.01| Preference_score: 0.27\n",
      "#user: 5 iter800: loss: -0.34| logit_loss: 0.00| shift_loss: 0.01| Preference_score: 0.35\n",
      "#user: 5 iter900: loss: -0.34| logit_loss: 0.00| shift_loss: 0.01| Preference_score: 0.35\n",
      "Shift-predictor Loaded!\n",
      "Working on Recommender Models!!\n",
      "Loading Rs Model Weights!!!!\n",
      "#user: 5 iter0: loss: 1.78| logit_loss: 1.85| shift_loss: 0.49| Preference_score: -1.11\n",
      "#user: 5 iter100: loss: -1.99| logit_loss: 0.03| shift_loss: 0.02| Preference_score: 2.01\n",
      "#user: 5 iter200: loss: -2.28| logit_loss: 0.01| shift_loss: 0.02| Preference_score: 2.30\n",
      "#user: 5 iter300: loss: -2.78| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 2.79\n",
      "#user: 5 iter400: loss: -2.87| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 2.88\n",
      "#user: 5 iter500: loss: -2.89| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 2.90\n",
      "#user: 5 iter600: loss: -2.92| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 2.93\n",
      "#user: 5 iter700: loss: -2.89| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 2.90\n",
      "#user: 5 iter800: loss: -3.43| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 3.44\n",
      "#user: 5 iter900: loss: -3.22| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 3.23\n",
      "Shift-predictor Loaded!\n",
      "Working on Recommender Models!!\n",
      "Loading Rs Model Weights!!!!\n",
      "#user: 5 iter0: loss: 3.87| logit_loss: 1.87| shift_loss: 0.49| Preference_score: -3.20\n",
      "#user: 5 iter100: loss: 1.17| logit_loss: 0.02| shift_loss: 0.05| Preference_score: -1.12\n",
      "#user: 5 iter200: loss: 0.97| logit_loss: 0.02| shift_loss: 0.02| Preference_score: -0.95\n",
      "#user: 5 iter300: loss: 0.83| logit_loss: 0.01| shift_loss: 0.02| Preference_score: -0.81\n",
      "#user: 5 iter400: loss: 0.69| logit_loss: 0.01| shift_loss: 0.01| Preference_score: -0.68\n",
      "#user: 5 iter500: loss: 0.73| logit_loss: 0.01| shift_loss: 0.01| Preference_score: -0.71\n",
      "#user: 5 iter600: loss: 0.61| logit_loss: 0.01| shift_loss: 0.01| Preference_score: -0.60\n",
      "#user: 5 iter700: loss: 0.32| logit_loss: 0.01| shift_loss: 0.01| Preference_score: -0.31\n",
      "#user: 5 iter800: loss: 0.50| logit_loss: 0.01| shift_loss: 0.01| Preference_score: -0.49\n",
      "#user: 5 iter900: loss: 0.35| logit_loss: 0.01| shift_loss: 0.01| Preference_score: -0.34\n",
      "Shift-predictor Loaded!\n",
      "Working on Recommender Models!!\n",
      "Loading Rs Model Weights!!!!\n",
      "#user: 5 iter0: loss: 2.91| logit_loss: 1.85| shift_loss: 0.50| Preference_score: -2.22\n",
      "#user: 5 iter100: loss: -1.08| logit_loss: 0.03| shift_loss: 0.02| Preference_score: 1.11\n",
      "#user: 5 iter200: loss: -2.14| logit_loss: 0.02| shift_loss: 0.02| Preference_score: 2.16\n",
      "#user: 5 iter300: loss: -2.46| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 2.47\n",
      "#user: 5 iter400: loss: -2.80| logit_loss: 0.01| shift_loss: 0.02| Preference_score: 2.82\n",
      "#user: 5 iter500: loss: -2.91| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 2.92\n",
      "#user: 5 iter600: loss: -2.81| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 2.83\n",
      "#user: 5 iter700: loss: -3.12| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 3.13\n",
      "#user: 5 iter800: loss: -3.19| logit_loss: 0.01| shift_loss: 0.01| Preference_score: 3.20\n",
      "#user: 5 iter900: loss: -3.20| logit_loss: 0.00| shift_loss: 0.01| Preference_score: 3.21\n"
     ]
    }
   ],
   "source": [
    "#### Defining models for GAN disentaglement\n",
    "from constants import DEFORMATOR_TYPE_DICT\n",
    "from latent_deformator import LatentDeformator, DeformatorType\n",
    "from latent_shift_predictor import LatentShiftPredictorV3\n",
    "from latent_shift_predictor import LatentReconstructor\n",
    "\n",
    "\n",
    "## Define Static Variable\n",
    "CE_loss = nn.CrossEntropyLoss()\n",
    "# deformator_type = 'my_case'\n",
    "deformator_type = 'my_case'\n",
    "\n",
    "deformator_random_init = True\n",
    "shift_predictor_size = None\n",
    "shift_predictor_type = 'ResNet'\n",
    "shift_distribution_key = 0 ## 0 for normal || 1 for uniform\n",
    "seed = 2\n",
    "device = 2\n",
    "multi_gpu = False\n",
    "\n",
    "\n",
    "shift_scale = 6.0\n",
    "min_shift = 0.5\n",
    "shift_distribution_type = shift_distribution_key\n",
    "\n",
    "deformator_lr = 0.01\n",
    "shift_predictor_lr = 0.01\n",
    "n_steps = int(1000)\n",
    "batch_size = 6\n",
    "\n",
    "directions_count = batch_size\n",
    "max_latent_dim = 512\n",
    "\n",
    "label_weight = 1.0\n",
    "shift_weight = 0.25\n",
    "print_Every = 100\n",
    "\n",
    "\n",
    "## takes in a list of [direction_count]\n",
    "\n",
    "# for all those batches fetch the direction_count index\n",
    "# generate a shift vector denotes how much you want to shift those directions corresponding target_indices\n",
    "# manually normalize every shift magnitude above minimum shift\n",
    "# for each of those batch size images (create a vector that basically tells for which images you will\n",
    "# continued: modify which target indice and by how much)\n",
    "\n",
    "def make_shifts(latent_dim):\n",
    "    target_indices = torch.randperm(\n",
    "        directions_count)[:batch_size].cuda()\n",
    "    if shift_distribution_type == 0:\n",
    "        shifts = torch.randn(target_indices.shape, device='cuda')\n",
    "    elif shift_distribution_type == 1:\n",
    "        shifts = 2.0 * torch.rand(target_indices.shape, device='cuda') - 1.0\n",
    "\n",
    "    shifts = shift_scale * shifts\n",
    "    shifts[(shifts < min_shift) & (shifts > 0)] = min_shift\n",
    "    shifts[(shifts > -min_shift) & (shifts < 0)] = -min_shift\n",
    "\n",
    "    try:\n",
    "        latent_dim[0]\n",
    "        latent_dim = list(latent_dim)\n",
    "    except Exception:\n",
    "        latent_dim = [latent_dim]\n",
    "\n",
    "    z_shift = torch.zeros([batch_size] + latent_dim, device='cuda')\n",
    "    for i, (index, val) in enumerate(zip(target_indices, shifts)):\n",
    "        z_shift[i][index] += val\n",
    "\n",
    "    return target_indices, shifts, z_shift\n",
    "\n",
    "\n",
    "\n",
    "def make_shifts_1(shifts, z):\n",
    "    \"\"\"\n",
    "    shifts shape should be (batch_size, 1)\n",
    "    z shape should be (batch_size, 512) ProGAN\n",
    "    \"\"\"\n",
    "    target_indices = torch.randperm(\n",
    "        batch_size)[:batch_size].cuda()\n",
    "\n",
    "    # shifts = shifts[target_indices]\n",
    "    shifts = shift_scale * shifts\n",
    "    shifts[(shifts < min_shift) & (shifts > 0)] = min_shift\n",
    "    shifts[(shifts > -min_shift) & (shifts < 0)] = -min_shift\n",
    "\n",
    "    return shifts\n",
    "\n",
    "\n",
    "def calculate_percept_loss(img_shifted):\n",
    "    k = img_shifted\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(len(img_shifted)):\n",
    "        fix_image = img_shifted[i].unsqueeze(dim=0).repeat(batch_size-1,1,1,1)\n",
    "        rest_images = torch.cat([img_shifted[0:i], img_shifted[i+1:]], dim=0)\n",
    "        perceptual_loss = percept(fix_image,rest_images).mean()\n",
    "        total_loss += perceptual_loss\n",
    "\n",
    "    return total_loss/len(img_shifted)\n",
    "\n",
    "\n",
    "def log_train(step, should_print=True, stats=()):\n",
    "    if should_print:\n",
    "        out_text = '{}% [step {}]'.format(int(100 * step / n_steps), step)\n",
    "        for named_value in stats:\n",
    "            out_text += (' | {}: {:.2f}'.format(*named_value))\n",
    "        print(out_text)\n",
    "\n",
    "## for unpref images\n",
    "#preferred_images, unpref_images, u, _, _ = next(iter(data_loader_train))\n",
    "\n",
    "\n",
    "### for pref images\n",
    "#unpref_images, _, _, _, _ = next(iter(data_loader_train))\n",
    "\n",
    "num_of_users = 5\n",
    "u = torch.LongTensor(random.sample(range(0, usernum), num_of_users))\n",
    "\n",
    "all_user_pref = []\n",
    "base_user_pref = []\n",
    "item_user_pref = []\n",
    "item_images = []\n",
    "base_gen_images = []\n",
    "shifted_images = []\n",
    "\n",
    "# for u_i in u:\n",
    "for u_i in range(10):\n",
    "\n",
    "\n",
    "    if shift_predictor_type == 'ResNet':\n",
    "        shift_predictor = LatentShiftPredictorV3(\n",
    "            batch_size, shift_predictor_size).cuda()    \n",
    "    elif shift_predictor_type == 'LeNet':\n",
    "        shift_predictor = LeNetShiftPredictor(\n",
    "            batch_size, 1 if gan_type == 'SN_MNIST' else 3).cuda()\n",
    "\n",
    "    latent_recon = LatentReconstructor(batch_size).cuda()\n",
    "    print('Shift-predictor Loaded!')\n",
    "\n",
    "\n",
    "\n",
    "    print('Working on Recommender Models!!')\n",
    "\n",
    "    latent_rs_model = 100\n",
    "\n",
    "    def init_weights_rs(m):\n",
    "        if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "    rs_model = nn.DataParallel(recsys_models.pthDVBPR(latent_rs_model))\n",
    "    rs_model.apply(init_weights_rs)\n",
    "\n",
    "    print('Loading Rs Model Weights!!!!')\n",
    "    rs_path = '../AIP/models/ckpt/Newamazon_K100_19.tar'\n",
    "    rs_Weights = torch.load(rs_path)\n",
    "\n",
    "    rs_model.load_state_dict(rs_Weights['model_state_dict'])\n",
    "    rs_model.eval()\n",
    "\n",
    "    thetau = torch.from_numpy(rs_Weights['U']).cuda()\n",
    "\n",
    "    G.cuda().eval()\n",
    "\n",
    "    dlatent=torch.randn((1,512),requires_grad=True,device='cuda')\n",
    "    dlatent_mean, dlatent_std = dlatent.mean(), dlatent.std()\n",
    "    dlatent = (dlatent-dlatent_mean)/dlatent_std\n",
    "\n",
    "    dlatent_img = scale_percept(G(dlatent, [0,0]))\n",
    "\n",
    "    user_nu = u.cuda()\n",
    "    ### User Preference\n",
    "    rs_in1 = dlatent_img\n",
    "    rs_feat1 = rs_model(rs_in1)\n",
    "    # Preference_score = torch.log(torch.sigmoid(torch.mul(thetau[user_nu],rs_feat).sum(1))).mean()\n",
    "    Preference_score1 = torch.mul(torch.index_select(thetau, dim=0, index=user_nu),rs_feat1).sum(1).mean()\n",
    "\n",
    "    dlatent_shift=torch.randn((batch_size,512),requires_grad=True,device='cuda')\n",
    "    shift_opt = torch.optim.Adam([dlatent_shift], lr=deformator_lr)\n",
    "    shift_predictor.cuda().train()\n",
    "    shift_predictor_opt = torch.optim.Adam(\n",
    "        shift_predictor.parameters(), lr=shift_predictor_lr)\n",
    "\n",
    "    dlatent = dlatent.detach()\n",
    "    n_steps = 1000\n",
    "    for step in range(0, n_steps, 1): \n",
    "        G.zero_grad()\n",
    "        ##### Optimizing dlatent shift #####\n",
    "        shift_opt.zero_grad()\n",
    "        shift_predictor_opt.zero_grad()\n",
    "\n",
    "        target_indices = torch.randperm(batch_size)[:batch_size].cuda()\n",
    "        max_shift = 0.5\n",
    "        shifts = max_shift * dlatent_shift[target_indices]\n",
    "        shifts = torch.clamp(shifts, min=-max_shift, max=max_shift)\n",
    "        z_shift = dlatent.detach() + shifts\n",
    "\n",
    "\n",
    "        z_shift_mean, z_shift_std = z_shift.mean(), z_shift.std()\n",
    "        z_shift = (z_shift-z_shift_mean)/z_shift_std\n",
    "\n",
    "        # Deformation\n",
    "        imgs = scale_percept(G(z_shift,[0,0]))\n",
    "        logits, predicted_shifts = shift_predictor(imgs)\n",
    "        logit_loss = label_weight * CE_loss(logits, target_indices)\n",
    "        shift_loss = torch.mean(torch.abs(predicted_shifts - shifts))\n",
    "\n",
    "        ### User Preference\n",
    "        rs_in = imgs\n",
    "        rs_feat = rs_model(rs_in)\n",
    "        # Preference_score = torch.log(torch.sigmoid(torch.mul(thetau[user_nu],rs_feat).sum(1))).mean()\n",
    "        Preference_score = torch.mul(torch.index_select(thetau, dim=0, index=user_nu),rs_feat.unsqueeze(dim=1)).mean(1).sum(1)\n",
    "\n",
    "        # total loss\n",
    "        loss = 0.1*logit_loss + shift_loss + (-1*Preference_score.mean())\n",
    "        # loss = -1*Preference_score\n",
    "        loss.backward()\n",
    "\n",
    "        shift_opt.step()\n",
    "\n",
    "        shift_predictor_opt.step()\n",
    "        # update statistics trackers\n",
    "        if step%100==0:\n",
    "            print(\"#user: {} iter{}: loss: {:.2f}| logit_loss: {:.2f}| shift_loss: {:.2f}| Preference_score: {:.2f}\".format(len(user_nu), step,loss.item(),logit_loss.item(),shift_loss.item(),Preference_score.mean().item()))\n",
    "\n",
    "    user_nu_preference = Preference_score.cpu().detach().unsqueeze(dim=0)\n",
    "    all_user_pref.append(user_nu_preference)\n",
    "\n",
    "    user_base_preference = Preference_score1.cpu().detach().unsqueeze(dim=0)\n",
    "    base_user_pref.append(user_base_preference)\n",
    "\n",
    "    base_gen_images.append(dlatent_img.detach().cpu())\n",
    "    shifted_images.append(imgs.detach().cpu())\n",
    "\n",
    "torch.save(all_user_pref, 'raw_data_exp/multi_user/preference_score_U_{}.pt'.format(num_of_users))\n",
    "torch.save(base_user_pref, 'raw_data_exp/multi_user/base_preference_score_U_{}.pt'.format(num_of_users))\n",
    "torch.save(base_gen_images, 'raw_data_exp/multi_user/base_images_U_{}.pt'.format(num_of_users))\n",
    "torch.save(shifted_images, 'raw_data_exp/multi_user/shifted_images_U_{}.pt'.format(num_of_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rs_attack",
   "language": "python",
   "name": "rs_attack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
